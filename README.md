# mini_llm

> A minimal, fromâ€‘scratch implementation of a tokenizer + transformerâ€‘based language model, designed to learn *Cervantesâ€‘style* text generation.

---

## âœ¨ Overview

**mini_llm** is a lightweight project that walks through the full pipeline of building a small language model:

1. **Text preprocessing and tokenization** (BPE)
2. **Training a transformerâ€‘based LLM**
3. **Autoregressive text generation**

---

## ğŸ§© Tokenizer

The tokenizer is implemented from scratch and provides:

* **Byte Pair Encoding (BPE)** algorithm
* **Regexâ€‘based preprocessing**, using the same pattern as GPTâ€‘4
* Full workflow support:

  * Training
  * Encoding / decoding
  * Saving and loading tokenizer state

This allows the tokenizer to be trained independently and reused across experiments.

---

## ğŸ¤– Language Model (LLM)

The LLM is a **transformerâ€‘based autoregressive model** trained to generate text in the style of **Miguel de Cervantes**.

Features:

* Decoderâ€‘only transformer architecture
* Endâ€‘toâ€‘end training from raw text
* Characteristic literary text generation

> âš ï¸ **Note**: Model checkpoint saving/loading is **not yet implemented**, but is planned as a future improvement.

---

## ğŸš€ Training the Model

First we need to train out tokenizer:

```bash
python dataset.py --train True --train_dataset datasets/cervantes.txt --vocab_size 256 --encode_dataset datasets/cervantes.txt
```

The program will automatically detect the tokenizer model generated during training. If you want to specify a model, you can run:

```bash
python dataset.py -encode_dataset datasets/cervantes.txt -m tokenizer_models/cervantes256.model
```

> Note that the training dataset and the dataset you want to encode for the LLM training can be different.

To train the language model:

```bash
python train.py
```

Training configuration and hyperparameters can be modified directly in the training script.

---

## ğŸ§ª Testing

Tokenizer functionality is covered by unit tests.

To run the tests:

```bash
pytest -v
```

---

## ğŸ› ï¸ Roadmap

Planned improvements:

* [ ] Model checkpoint save/load support
* [ ] Inferenceâ€‘only generation script
* [ ] Training metrics logging
* [ ] Improved documentation and examples

---