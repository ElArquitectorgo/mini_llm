# mini_llm

> A minimal, fromâ€‘scratch implementation of a tokenizer + transformerâ€‘based language model, designed to learn *Cervantesâ€‘style* text generation.

---

## âœ¨ Overview

**mini_llm** is a lightweight project that walks through the full pipeline of building a small language model:

1. **Text preprocessing and tokenization** (BPE)
2. **Training a transformerâ€‘based LLM**
3. **Autoregressive text generation**

---

## ğŸ§© Tokenizer

The tokenizer is implemented from scratch and provides:

* **Byte Pair Encoding (BPE)** algorithm
* **Regexâ€‘based preprocessing**, using the same pattern as GPTâ€‘4
* Full workflow support:

  * Training
  * Encoding / decoding
  * Saving and loading tokenizer state

This allows the tokenizer to be trained independently and reused across experiments.

---

## ğŸ¤– Language Model (LLM)

The LLM is a **transformerâ€‘based autoregressive model** trained to generate text in the style of **Miguel de Cervantes**.

Features:

* Decoderâ€‘only transformer architecture
* Endâ€‘toâ€‘end training from raw text
* Characteristic literary text generation

> âš ï¸ **Note**: Model checkpoint saving/loading is **not yet implemented**, but is planned as a future improvement.

---

## ğŸš€ Training the Model

To train the language model:

```bash
cd src
python train.py
```

Training configuration and hyperparameters can be modified directly in the training script.

---

## ğŸ§ª Testing

Tokenizer functionality is covered by unit tests.

To run the tests:

```bash
pytest -v
```

---

## ğŸ› ï¸ Roadmap

Planned improvements:

* [ ] Model checkpoint save/load support
* [ ] Inferenceâ€‘only generation script
* [ ] Training metrics logging
* [ ] Improved documentation and examples

---